{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ-wFa9tdRBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Copyright (c) Microsoft. All rights reserved.\n",
        "# Licensed under the MIT license. See LICENSE.md file in the project root for full license information.\n",
        "#\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import logging\n",
        "import random as rnd\n",
        "from collections import namedtuple\n",
        "\n",
        "from PIL import Image\n",
        "from rect_util import Rect\n",
        "import img_util as imgu\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_summary(train_data_reader, val_data_reader, test_data_reader):\n",
        "    '''\n",
        "    Summarize the data in a tabular format.\n",
        "    '''\n",
        "    emotion_count = train_data_reader.emotion_count\n",
        "    emotin_header = ['neutral', 'happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear', 'contempt']\n",
        "\n",
        "    logging.info(\"{0}\\t{1}\\t{2}\\t{3}\".format(\"\".ljust(10), \"Train\", \"Val\", \"Test\"))\n",
        "    for index in range(emotion_count):\n",
        "        logging.info(\"{0}\\t{1}\\t{2}\\t{3}\".format(emotin_header[index].ljust(10), \n",
        "                     train_data_reader.per_emotion_count[index], \n",
        "                     val_data_reader.per_emotion_count[index], \n",
        "                     test_data_reader.per_emotion_count[index]))\n",
        "\n",
        "class FERPlusParameters():\n",
        "    '''\n",
        "    FER+ reader parameters\n",
        "    '''\n",
        "    def __init__(self, target_size, width, height, training_mode = \"majority\", determinisitc = False, shuffle = True):\n",
        "        self.target_size   = target_size\n",
        "        self.width         = width\n",
        "        self.height        = height\n",
        "        self.training_mode = training_mode\n",
        "        self.determinisitc = determinisitc\n",
        "        self.shuffle       = shuffle\n",
        "                     \n",
        "class FERPlusReader(object):\n",
        "    '''\n",
        "    A custom reader for FER+ dataset that support multiple modes as described in:\n",
        "        https://arxiv.org/abs/1608.01041\n",
        "    '''\n",
        "    @classmethod\n",
        "    def create(cls, base_folder, sub_folders, label_file_name, parameters):\n",
        "        '''\n",
        "        Factory function that create an instance of FERPlusReader and load the data form disk.\n",
        "        '''\n",
        "        reader = cls(base_folder, sub_folders, label_file_name, parameters)\n",
        "        reader.load_folders(parameters.training_mode)\n",
        "        return reader\n",
        "        \n",
        "    def __init__(self, base_folder, sub_folders, label_file_name, parameters):\n",
        "        '''\n",
        "        Each sub_folder contains the image files and a csv file for the corresponding label. The read iterate through\n",
        "        all the sub_folders and aggregate all the images and their corresponding labels.        \n",
        "        '''\n",
        "        self.base_folder     = base_folder\n",
        "        self.sub_folders     = sub_folders\n",
        "        self.label_file_name = label_file_name\n",
        "        self.emotion_count   = parameters.target_size\n",
        "        self.width           = parameters.width\n",
        "        self.height          = parameters.height\n",
        "        self.shuffle         = parameters.shuffle\n",
        "        self.training_mode   = parameters.training_mode\n",
        "\n",
        "        # data augmentation parameters.determinisitc\n",
        "        if parameters.determinisitc:\n",
        "            self.max_shift = 0.0\n",
        "            self.max_scale = 1.0\n",
        "            self.max_angle = 0.0\n",
        "            self.max_skew = 0.0\n",
        "            self.do_flip = False\n",
        "        else:\n",
        "            self.max_shift = 0.08\n",
        "            self.max_scale = 1.05\n",
        "            self.max_angle = 20.0\n",
        "            self.max_skew = 0.05\n",
        "            self.do_flip = True\n",
        "        \n",
        "        self.data              = None\n",
        "        self.per_emotion_count = None\n",
        "        self.batch_start       = 0\n",
        "        self.indices           = 0\n",
        "\n",
        "        self.A, self.A_pinv = imgu.compute_norm_mat(self.width, self.height)\n",
        "        \n",
        "    def has_more(self):\n",
        "        '''\n",
        "        Return True if there is more min-batches.\n",
        "        '''\n",
        "        if self.batch_start < len(self.data):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def reset(self):\n",
        "        '''\n",
        "        Start from beginning for the new epoch.\n",
        "        '''\n",
        "        self.batch_start = 0\n",
        "\n",
        "    def size(self):\n",
        "        '''\n",
        "        Return the number of images read by this reader.\n",
        "        '''\n",
        "        return len(self.data)\n",
        "        \n",
        "    def next_minibatch(self, batch_size):\n",
        "        '''\n",
        "        Return the next mini-batch, we do data augmentation during constructing each mini-batch.\n",
        "        '''\n",
        "        data_size = len(self.data)\n",
        "        batch_end = min(self.batch_start + batch_size, data_size)\n",
        "        current_batch_size = batch_end - self.batch_start\n",
        "        if current_batch_size < 0:\n",
        "            raise Exception('Reach the end of the training data.')\n",
        "        \n",
        "        inputs = np.empty(shape=(current_batch_size, 1, self.width, self.height), dtype=np.float32)\n",
        "        targets = np.empty(shape=(current_batch_size, self.emotion_count), dtype=np.float32)\n",
        "        for idx in range(self.batch_start, batch_end):\n",
        "            index = self.indices[idx]\n",
        "            distorted_image = imgu.distort_img(self.data[index][1], \n",
        "                                               self.data[index][3], \n",
        "                                               self.width, \n",
        "                                               self.height, \n",
        "                                               self.max_shift, \n",
        "                                               self.max_scale, \n",
        "                                               self.max_angle, \n",
        "                                               self.max_skew, \n",
        "                                               self.do_flip)\n",
        "            final_image = imgu.preproc_img(distorted_image, A=self.A, A_pinv=self.A_pinv)\n",
        "\n",
        "            inputs[idx-self.batch_start]    = final_image\n",
        "            targets[idx-self.batch_start,:] = self._process_target(self.data[index][2])\n",
        "\n",
        "        self.batch_start += current_batch_size\n",
        "        return inputs, targets, current_batch_size\n",
        "        \n",
        "    def load_folders(self, mode):\n",
        "        '''\n",
        "        Load the actual images from disk. While loading, we normalize the input data.\n",
        "        '''\n",
        "        self.reset()\n",
        "        self.data = []\n",
        "        self.per_emotion_count = np.zeros(self.emotion_count, dtype=np.int)\n",
        "        \n",
        "        for folder_name in self.sub_folders: \n",
        "            logging.info(\"Loading %s\" % (os.path.join(self.base_folder, folder_name)))\n",
        "            folder_path = os.path.join(self.base_folder, folder_name)\n",
        "            in_label_path = os.path.join(folder_path, self.label_file_name)\n",
        "            with open(in_label_path) as csvfile: \n",
        "                emotion_label = csv.reader(csvfile) \n",
        "                for row in emotion_label: \n",
        "                    # load the image\n",
        "                    image_path = os.path.join(folder_path, row[0])\n",
        "                    image_data = Image.open(image_path)\n",
        "                    image_data.load()\n",
        "\n",
        "                    # face rectangle \n",
        "                    box = list(map(int, row[1][1:-1].split(',')))\n",
        "                    face_rc = Rect(box)\n",
        "\n",
        "                    emotion_raw = list(map(float, row[2:len(row)]))\n",
        "                    emotion = self._process_data(emotion_raw, mode) \n",
        "                    idx = np.argmax(emotion)\n",
        "                    if idx < self.emotion_count: # not unknown or non-face \n",
        "                        emotion = emotion[:-2]\n",
        "                        emotion = [float(i)/sum(emotion) for i in emotion]\n",
        "                        self.data.append((image_path, image_data, emotion, face_rc))\n",
        "                        self.per_emotion_count[idx] += 1\n",
        "        \n",
        "        self.indices = np.arange(len(self.data))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "    \n",
        "    def _process_target(self, target):\n",
        "        '''\n",
        "        Based on https://arxiv.org/abs/1608.01041 the target depend on the training mode.\n",
        "\n",
        "        Majority or crossentropy: return the probability distribution generated by \"_process_data\"\n",
        "        Probability: pick one emotion based on the probability distribtuion.\n",
        "        Multi-target: \n",
        "        '''\n",
        "        if self.training_mode == 'majority' or self.training_mode == 'crossentropy': \n",
        "            return target\n",
        "        elif self.training_mode == 'probability': \n",
        "            idx             = np.random.choice(len(target), p=target) \n",
        "            new_target      = np.zeros_like(target)\n",
        "            new_target[idx] = 1.0\n",
        "            return new_target\n",
        "        elif self.training_mode == 'multi_target': \n",
        "            new_target = np.array(target) \n",
        "            new_target[new_target>0] = 1.0\n",
        "            epsilon = 0.001     # add small epsilon in order to avoid ill-conditioned computation\n",
        "            return (1-epsilon)*new_target + epsilon*np.ones_like(target)\n",
        "\n",
        "    def _process_data(self, emotion_raw, mode):\n",
        "        '''\n",
        "        Based on https://arxiv.org/abs/1608.01041, we process the data differently depend on the training mode:\n",
        "\n",
        "        Majority: return the emotion that has the majority vote, or unknown if the count is too little.\n",
        "        Probability or Crossentropty: convert the count into probability distribution.abs\n",
        "        Multi-target: treat all emotion with 30% or more votes as equal.\n",
        "        '''        \n",
        "        size = len(emotion_raw)\n",
        "        emotion_unknown     = [0.0] * size\n",
        "        emotion_unknown[-2] = 1.0\n",
        "\n",
        "        # remove emotions with a single vote (outlier removal) \n",
        "        for i in range(size):\n",
        "            if emotion_raw[i] < 1.0 + sys.float_info.epsilon:\n",
        "                emotion_raw[i] = 0.0\n",
        "\n",
        "        sum_list = sum(emotion_raw)\n",
        "        emotion = [0.0] * size \n",
        "\n",
        "        if mode == 'majority': \n",
        "            # find the peak value of the emo_raw list \n",
        "            maxval = max(emotion_raw) \n",
        "            if maxval > 0.5*sum_list: \n",
        "                emotion[np.argmax(emotion_raw)] = maxval \n",
        "            else: \n",
        "                emotion = emotion_unknown   # force setting as unknown \n",
        "        elif (mode == 'probability') or (mode == 'crossentropy'):\n",
        "            sum_part = 0\n",
        "            count = 0\n",
        "            valid_emotion = True\n",
        "            while sum_part < 0.75*sum_list and count < 3 and valid_emotion:\n",
        "                maxval = max(emotion_raw) \n",
        "                for i in range(size): \n",
        "                    if emotion_raw[i] == maxval: \n",
        "                        emotion[i] = maxval\n",
        "                        emotion_raw[i] = 0\n",
        "                        sum_part += emotion[i]\n",
        "                        count += 1\n",
        "                        if i >= 8:  # unknown or non-face share same number of max votes \n",
        "                            valid_emotion = False\n",
        "                            if sum(emotion) > maxval:   # there have been other emotions ahead of unknown or non-face\n",
        "                                emotion[i] = 0\n",
        "                                count -= 1\n",
        "                            break\n",
        "            if sum(emotion) <= 0.5*sum_list or count > 3: # less than 50% of the votes are integrated, or there are too many emotions, we'd better discard this example\n",
        "                emotion = emotion_unknown   # force setting as unknown \n",
        "        elif mode == 'multi_target':\n",
        "            threshold = 0.3\n",
        "            for i in range(size): \n",
        "                if emotion_raw[i] >= threshold*sum_list: \n",
        "                    emotion[i] = emotion_raw[i] \n",
        "            if sum(emotion) <= 0.5 * sum_list: # less than 50% of the votes are integrated, we discard this example \n",
        "                emotion = emotion_unknown   # set as unknown \n",
        "                                \n",
        "        return [float(i)/sum(emotion) for i in emotion]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}