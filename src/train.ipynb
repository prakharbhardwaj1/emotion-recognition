{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ-wFa9tdRBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Copyright (c) Microsoft. All rights reserved.\n",
        "# Licensed under the MIT license. See LICENSE.md file in the project root for full license information.\n",
        "#\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import os\n",
        "import math\n",
        "import csv\n",
        "import argparse\n",
        "import numpy as np\n",
        "import logging\n",
        "\n",
        "from models import *\n",
        "from ferplus import *\n",
        "\n",
        "import cntk as ct\n",
        "\n",
        "emotion_table = {'neutral'  : 0, \n",
        "                 'happiness': 1, \n",
        "                 'surprise' : 2, \n",
        "                 'sadness'  : 3, \n",
        "                 'anger'    : 4, \n",
        "                 'disgust'  : 5, \n",
        "                 'fear'     : 6, \n",
        "                 'contempt' : 7}\n",
        "\n",
        "# List of folders for training, validation and test.\n",
        "train_folders = ['FER2013Train']\n",
        "valid_folders = ['FER2013Valid'] \n",
        "test_folders  = ['FER2013Test']\n",
        "\n",
        "def cost_func(training_mode, prediction, target):\n",
        "    '''\n",
        "    We use cross entropy in most mode, except for the multi-label mode, which require treating\n",
        "    multiple labels exactly the same.\n",
        "    '''\n",
        "    train_loss = None\n",
        "    if training_mode == 'majority' or training_mode == 'probability' or training_mode == 'crossentropy': \n",
        "        # Cross Entropy.\n",
        "        train_loss = ct.negate(ct.reduce_sum(ct.element_times(target, ct.log(prediction)), axis=-1))\n",
        "    elif training_mode == 'multi_target':\n",
        "        train_loss = ct.negate(ct.log(ct.reduce_max(ct.element_times(target, prediction), axis=-1)))\n",
        "\n",
        "    return train_loss\n",
        "    \n",
        "def main(base_folder, training_mode='majority', model_name='VGG13', max_epochs = 100):\n",
        "\n",
        "    # create needed folders.\n",
        "    output_model_path   = os.path.join(base_folder, R'models')\n",
        "    output_model_folder = os.path.join(output_model_path, model_name + '_' + training_mode)\n",
        "    if not os.path.exists(output_model_folder):\n",
        "        os.makedirs(output_model_folder)\n",
        "\n",
        "    # creating logging file \n",
        "    logging.basicConfig(filename = os.path.join(output_model_folder, \"train.log\"), filemode = 'w', level = logging.INFO)\n",
        "    logging.getLogger().addHandler(logging.StreamHandler())\n",
        "\n",
        "    logging.info(\"Starting with training mode {} using {} model and max epochs {}.\".format(training_mode, model_name, max_epochs))\n",
        "\n",
        "    # create the model\n",
        "    num_classes = len(emotion_table)\n",
        "    model       = build_model(num_classes, model_name)\n",
        "\n",
        "    # set the input variables.\n",
        "    input_var = ct.input((1, model.input_height, model.input_width), np.float32)\n",
        "    label_var = ct.input((num_classes), np.float32)\n",
        "    \n",
        "    # read FER+ dataset.\n",
        "    logging.info(\"Loading data...\")\n",
        "    train_params        = FERPlusParameters(num_classes, model.input_height, model.input_width, training_mode, False)\n",
        "    test_and_val_params = FERPlusParameters(num_classes, model.input_height, model.input_width, \"majority\", True)\n",
        "\n",
        "    train_data_reader   = FERPlusReader.create(base_folder, train_folders, \"label.csv\", train_params)\n",
        "    val_data_reader     = FERPlusReader.create(base_folder, valid_folders, \"label.csv\", test_and_val_params)\n",
        "    test_data_reader    = FERPlusReader.create(base_folder, test_folders, \"label.csv\", test_and_val_params)\n",
        "    \n",
        "    # print summary of the data.\n",
        "    display_summary(train_data_reader, val_data_reader, test_data_reader)\n",
        "    \n",
        "    # get the probalistic output of the model.\n",
        "    z    = model.model(input_var)\n",
        "    pred = ct.softmax(z)\n",
        "    \n",
        "    epoch_size     = train_data_reader.size()\n",
        "    minibatch_size = 32\n",
        "\n",
        "    # Training config\n",
        "    lr_per_minibatch       = [model.learning_rate]*20 + [model.learning_rate / 2.0]*20 + [model.learning_rate / 10.0]\n",
        "    mm_time_constant       = -minibatch_size/np.log(0.9)\n",
        "    lr_schedule            = ct.learning_rate_schedule(lr_per_minibatch, unit=ct.UnitType.minibatch, epoch_size=epoch_size)\n",
        "    mm_schedule            = ct.momentum_as_time_constant_schedule(mm_time_constant)\n",
        "\n",
        "    # loss and error cost\n",
        "    train_loss = cost_func(training_mode, pred, label_var)\n",
        "    pe         = ct.classification_error(z, label_var)\n",
        "\n",
        "    # construct the trainer\n",
        "    learner = ct.momentum_sgd(z.parameters, lr_schedule, mm_schedule)\n",
        "    trainer = ct.Trainer(z, (train_loss, pe), learner)\n",
        "\n",
        "    # Get minibatches of images to train with and perform model training\n",
        "    max_val_accuracy    = 0.0\n",
        "    final_test_accuracy = 0.0\n",
        "    best_test_accuracy  = 0.0\n",
        "\n",
        "    logging.info(\"Start training...\")\n",
        "    epoch      = 0\n",
        "    best_epoch = 0\n",
        "    while epoch < max_epochs: \n",
        "        train_data_reader.reset()\n",
        "        val_data_reader.reset()\n",
        "        test_data_reader.reset()\n",
        "        \n",
        "        # Training \n",
        "        start_time = time.time()\n",
        "        training_loss = 0\n",
        "        training_accuracy = 0\n",
        "        while train_data_reader.has_more():\n",
        "            images, labels, current_batch_size = train_data_reader.next_minibatch(minibatch_size)\n",
        "\n",
        "            # Specify the mapping of input variables in the model to actual minibatch data to be trained with\n",
        "            trainer.train_minibatch({input_var : images, label_var : labels})\n",
        "\n",
        "            # keep track of statistics.\n",
        "            training_loss     += trainer.previous_minibatch_loss_average * current_batch_size\n",
        "            training_accuracy += trainer.previous_minibatch_evaluation_average * current_batch_size\n",
        "                \n",
        "        training_accuracy /= train_data_reader.size()\n",
        "        training_accuracy = 1.0 - training_accuracy\n",
        "        \n",
        "        # Validation\n",
        "        val_accuracy = 0\n",
        "        while val_data_reader.has_more():\n",
        "            images, labels, current_batch_size = val_data_reader.next_minibatch(minibatch_size)\n",
        "            val_accuracy += trainer.test_minibatch({input_var : images, label_var : labels}) * current_batch_size\n",
        "            \n",
        "        val_accuracy /= val_data_reader.size()\n",
        "        val_accuracy = 1.0 - val_accuracy\n",
        "        \n",
        "        # if validation accuracy goes higher, we compute test accuracy\n",
        "        test_run = False\n",
        "        if val_accuracy > max_val_accuracy:\n",
        "            best_epoch = epoch\n",
        "            max_val_accuracy = val_accuracy\n",
        "\n",
        "            trainer.save_checkpoint(os.path.join(output_model_folder, \"model_{}\".format(best_epoch)))\n",
        "\n",
        "            test_run = True\n",
        "            test_accuracy = 0\n",
        "            while test_data_reader.has_more():\n",
        "                images, labels, current_batch_size = test_data_reader.next_minibatch(minibatch_size)\n",
        "                test_accuracy += trainer.test_minibatch({input_var : images, label_var : labels}) * current_batch_size\n",
        "            \n",
        "            test_accuracy /= test_data_reader.size()\n",
        "            test_accuracy = 1.0 - test_accuracy\n",
        "            final_test_accuracy = test_accuracy\n",
        "            if final_test_accuracy > best_test_accuracy: \n",
        "                best_test_accuracy = final_test_accuracy\n",
        " \n",
        "        logging.info(\"Epoch {}: took {:.3f}s\".format(epoch, time.time() - start_time))\n",
        "        logging.info(\"  training loss:\\t{:e}\".format(training_loss))\n",
        "        logging.info(\"  training accuracy:\\t\\t{:.2f} %\".format(training_accuracy * 100))\n",
        "        logging.info(\"  validation accuracy:\\t\\t{:.2f} %\".format(val_accuracy * 100))\n",
        "        if test_run:\n",
        "            logging.info(\"  test accuracy:\\t\\t{:.2f} %\".format(test_accuracy * 100))\n",
        "            \n",
        "        epoch += 1\n",
        "\n",
        "    logging.info(\"\")\n",
        "    logging.info(\"Best validation accuracy:\\t\\t{:.2f} %, epoch {}\".format(max_val_accuracy * 100, best_epoch))\n",
        "    logging.info(\"Test accuracy corresponding to best validation:\\t\\t{:.2f} %\".format(final_test_accuracy * 100))\n",
        "    logging.info(\"Best test accuracy:\\t\\t{:.2f} %\".format(best_test_accuracy * 100))\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"-d\", \n",
        "                        \"--base_folder\", \n",
        "                        type = str, \n",
        "                        help = \"Base folder containing the training, validation and testing data.\", \n",
        "                        required = True)\n",
        "    parser.add_argument(\"-m\", \n",
        "                        \"--training_mode\", \n",
        "                        type = str,\n",
        "                        default='majority',\n",
        "                        help = \"Specify the training mode: majority, probability, crossentropy or multi_target.\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    main(args.base_folder, args.training_mode)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}